# Abstract Glossary

## Neural Architectures
* **LSTM RNN (Long Short-Term Memory):** A recurrent network with "memory gates" that captures long-term dependencies, solving the vanishing gradient problem of standard RNNs.
* **DNN (Feed Forward):** Standard deep networks without memory loops. The baseline this paper aims to outperform.

## Modeling Concepts
* **Acoustic Model:** The component that calculates the probability of a phoneme given an audio input.
* **HMM (Hidden Markov Model):** Statistical model used to handle the time evolution and transitions between phonemes.
* **CD (Context Dependent):** Modeling phonemes based on their neighbors (e.g., *triphones*) rather than in isolation.
* **CTC (Connectionist Temporal Classification):** A training technique that allows the network to learn sequences without needing precise frame-by-frame alignment labels.

## Optimization
* **Sequence Training:** Optimizing the model to minimize errors on the entire sentence sequence, rather than just individual frames.
* **Frame Stacking:** Concatenating multiple consecutive audio frames into a single input to give the network more immediate context.
* **Reduced Frame Rate:** Processing audio at longer time intervals (skipping frames) to significantly speed up decoding.


# Plan ideas
## Step 1: Data & Feature Preparation (the "Input")

The paper uses **Log-Mel Filterbank energy features**.

**Objective:** Load a small audio dataset (e.g., a short version of LibriSpeech or SpeechCommands to start simple).

**Actions:**

* Load the audio.
* Transform the waveform into a spectrogram (80 dimensions as in the paper).
* Visualize what the network "sees".

---

## Step 2: Implementing "Frame Stacking" (the "Fast")

This is the paperâ€™s specific technique to speed up the model.

**Objective:** Code the function that stacks frames and reduces the frame rate.

**Actions:**

* **Input:** A sequence of 100 frames (10 ms).
* **Processing:** Stack groups of 3 or 8 frames + sub-sampling (skip frames).
* **Output:** A shorter but richer sequence.

---

## Step 3: LSTM Architecture (the "Accurate")

Neural network construction using PyTorch.

**Objective:** Create the acoustic model.

**Actions:**

* **Layer 1:** Input Layer (size = 80 Ã— stacking factor).
* **Layer 2:** LSTM (with or without a linear projection layer for comparison).
* **Layer 3:** Fully Connected layer (projection to character classes).

---

## Step 4: CTC Loss & Training

This is where temporal alignment is handled without complex HMMs.

**Objective:** Connect the LSTM output to the CTC loss function.

**Actions:**

* Prepare labels (Text â†’ Integers).
* Configure `nn.CTCLoss`.
* Run a training loop for a few epochs.

---

## Step 5: Decoding & Extension (the Result)

**Objective:** Check whether the model "writes" what it hears.

**Actions:**

* **Greedy Decoder:** Select the maximum probability at each time step.
* **Extension (your idea):** Compare a model *with* frame stacking vs *without* frame stacking to evaluate speed gains and accuracy differences.




# Plan ideas

### 1. Introduction et contexte (â‰ˆ 1 min)

- Quâ€™est-ce que la reconnaissance automatique de la parole (ASR)
- RÃ´le du **modÃ¨le acoustique** dans un systÃ¨me ASR
- Limites des approches classiques :
  - GMM-HMM
  - DNN frame-based
- Motivation pour lâ€™utilisation des **RNN / LSTM** :
  - la parole est un signal **sÃ©quentiel et temporel**

**Message clÃ© :**  
> La reconnaissance vocale est un problÃ¨me sÃ©quentiel â†’ les RNN sont naturellement adaptÃ©s.

---

### 2. RNN et LSTM pour la reconnaissance vocale (â‰ˆ 1.5 min)

- Rappel rapide sur :
  - les RNN
  - les LSTM et le problÃ¨me du vanishing gradient
- DiffÃ©rence entre :
  - RNN unidirectionnels (faible latence)
  - RNN bidirectionnels (meilleure prÃ©cision)
- Architectures LSTM profondes (empilement de couches)

Lien avec lâ€™article :
- Architectures unidirectionnelles et bidirectionnelles
- Avantages des modÃ¨les profonds pour lâ€™ASR

---

### 3. Le problÃ¨me fondamental de lâ€™alignement (â‰ˆ 1 min)

- En ASR :
  - entrÃ©e : sÃ©quence de frames audio
  - sortie : sÃ©quence de phonÃ¨mes ou de mots
- Lâ€™alignement frame â†” label est **inconnu**
- Limites de lâ€™entraÃ®nement par cross-entropy classique
- NÃ©cessitÃ© dâ€™un mÃ©canisme dâ€™alignement automatique

---

### 4. Connectionist Temporal Classification (CTC) (â‰ˆ 2 min)

- Principe du **CTC**
- Introduction du **blank label**
- Plusieurs alignements valides pour une mÃªme transcription
- Utilisation du forwardâ€“backward algorithm
- DiffÃ©rences avec les modÃ¨les HMM traditionnels

Illustrations possibles :
- Exemple simple de mot (â€œCATâ€)
- Spikes de probabilitÃ©s dans le temps

**Message clÃ© :**  
> CTC permet dâ€™apprendre **quoi prÃ©dire et quand le prÃ©dire** sans alignement explicite.

---

### 5. AmÃ©liorations clÃ©s proposÃ©es dans lâ€™article Google (â‰ˆ 2 min)

#### 5.1 Frame stacking et subsampling
- RÃ©duction du nombre de frames traitÃ©es
- Stabilisation de lâ€™entraÃ®nement CTC
- AccÃ©lÃ©ration du dÃ©codage

#### 5.2 Context-Dependent Phones
- Phones context-indÃ©pendants vs context-dÃ©pendants
- Importance du contexte phonÃ©tique
- Gains significatifs en WER

#### 5.3 EntraÃ®nement discriminatif de sÃ©quence (sMBR)
- Limites des critÃ¨res CE et CTC
- Optimisation directe du Word Error Rate (WER)

**Message clÃ© :**  
> Les performances viennent autant de lâ€™architecture que des choix dâ€™ingÃ©nierie.

---

### 6. Extension et ouverture (â‰ˆ 1 min)

Exploration dâ€™une extension au-delÃ  de lâ€™article :
- ModÃ¨les acoustiques au niveau mot (CTC word models)
- Comparaison conceptuelle avec :
  - Attention-based models
  - Transformers / Conformers
- Discussion sur la pertinence actuelle de CTC

---

### 7. Conclusion (â‰ˆ 30 s)

- RÃ©sumÃ© des concepts clÃ©s
- Transition vers la dÃ©monstration pratique
- Importance de CTC comme brique fondamentale de lâ€™ASR moderne

---

## Plan du code demo (GitHub)

### Structure du dÃ©pÃ´t

```text
speech-recognition/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ mini_librispeech/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_lstm_ctc_training.ipynb
â”‚   â”œâ”€â”€ 03_decoding_and_results.ipynb
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ model.py
    â”œâ”€â”€ train.py
    â””â”€â”€ decode.py






# Recurrent Neural Network Acoustic Models for Speech Recognition

---

## ğŸ¯ Objectif global

Ce projet prÃ©sente une dÃ©monstration complÃ¨te et pÃ©dagogique de la **reconnaissance automatique de la parole (ASR)** Ã  lâ€™aide de **rÃ©seaux de neurones rÃ©currents (RNN / LSTM)**.  
Lâ€™approche suit le **chemin naturel des donnÃ©es**, depuis le signal audio brut jusquâ€™Ã  la transcription, en expliquant :

1. comment les donnÃ©es sont reprÃ©sentÃ©es,
2. comment les modÃ¨les sÃ©quentiels les traitent,
3. comment lâ€™alignement est appris,
4. quelles amÃ©liorations permettent dâ€™atteindre de bonnes performances.

---

# ğŸ§  PARTIE I â€“ DONNÃ‰ES ET REPRÃ‰SENTATION ACOUSTIQUE
*(VidÃ©o â€“ dÃ©but | Notebook 01)*

---

## 1. La reconnaissance automatique de la parole (ASR)

### ProblÃ©matique
- EntrÃ©e : signal audio continu
- Sortie : sÃ©quence discrÃ¨te de symboles (phonÃ¨mes, caractÃ¨res, mots)
- DÃ©fi principal : **nature temporelle et variable de la parole**

### ChaÃ®ne ASR simplifiÃ©e
Audio â†’ Features â†’ ModÃ¨le acoustique â†’ DÃ©codage â†’ Texte

Le projet se concentre sur le **modÃ¨le acoustique**.

---

## 2. DonnÃ©es audio et variabilitÃ© temporelle

### Points clÃ©s
- DurÃ©e variable des utterances
- Vitesse de parole non constante
- Coarticulation et contexte phonÃ©tique

ConsÃ©quence :
> Les modÃ¨les doivent **traiter des sÃ©quences de longueur variable** et **capturer le contexte temporel**.

---

## 3. Extraction des features acoustiques

### MÃ©thode
- DÃ©coupage du signal en fenÃªtres (25 ms, stride 10 ms)
- Extraction de **log-Mel filterbanks**
- Transformation du signal brut en reprÃ©sentation temps-frÃ©quence

### Justification
- ReprÃ©sentation plus stable que le signal brut
- InspirÃ©e de la perception humaine

ğŸ““ Notebook associÃ© :
- `01_data_exploration.ipynb`

---

# ğŸ§  PARTIE II â€“ MODÃ‰LISATION SÃ‰QUENTIELLE AVEC RNN / LSTM
*(VidÃ©o â€“ milieu | Notebook 02)*

---

## 4. Pourquoi des RNN pour la parole ?

### Limites des modÃ¨les frame-based
- HypothÃ¨se dâ€™indÃ©pendance entre frames
- Perte dâ€™information temporelle

### Avantage des RNN
- Traitement sÃ©quentiel
- MÃ©moire interne

---

## 5. Long Short-Term Memory (LSTM)

### Motivation
- RÃ©solution du problÃ¨me du vanishing gradient
- Capture des dÃ©pendances long-terme

### Architecture
- Cellule mÃ©moire
- Gates (input, forget, output)

---

## 6. ModÃ¨les unidirectionnels et bidirectionnels

### Unidirectionnel
- Utilise le passÃ© uniquement
- Faible latence
- AdaptÃ© aux systÃ¨mes temps rÃ©el

### Bidirectionnel
- Utilise passÃ© et futur
- Meilleure prÃ©cision
- Latence plus Ã©levÃ©e

ğŸ““ Notebook associÃ© :
- `02_lstm_ctc_training.ipynb`

---

# ğŸ§  PARTIE III â€“ LE PROBLÃˆME DE Lâ€™ALIGNEMENT

---

## 7. Absence dâ€™alignement explicite

### ProblÃ¨me fondamental
- Les transcriptions sont connues
- Lâ€™alignement frame â†” label est inconnu

### Limites de lâ€™approche classique
- Cross-entropy nÃ©cessite des alignements fixes
- Alignements coÃ»teux Ã  produire

---

# ğŸ§  PARTIE IV â€“ CONNECTIONIST TEMPORAL CLASSIFICATION (CTC)
*(VidÃ©o â€“ cÅ“ur conceptuel | Notebooks 02 & 03)*

---

## 8. Principe de CTC

### IdÃ©e centrale
- Introduire un **blank label**
- Autoriser plusieurs alignements valides
- Marginaliser sur tous les alignements possibles

### Apprentissage
- Forwardâ€“Backward algorithm
- Optimisation de la probabilitÃ© de la transcription correcte

---

## 9. DÃ©codage CTC

### MÃ©thode
- Suppression des rÃ©pÃ©titions
- Suppression des blanks
- DÃ©codage greedy (simplifiÃ©)

ğŸ““ Notebook associÃ© :
- `03_decoding_and_results.ipynb`

---

# ğŸ§  PARTIE V â€“ AMÃ‰LIORATIONS DES MODÃˆLES ACOUSTIQUES
*(VidÃ©o â€“ approfondissement | Notebooks 01â€“03)*

---

## 10. Frame stacking et subsampling

### Motivation
- RÃ©duction du nombre de frames
- Stabilisation de lâ€™entraÃ®nement CTC
- AccÃ©lÃ©ration du calcul

### ImplÃ©mentation
- ConcatÃ©nation de plusieurs frames
- Sous-Ã©chantillonnage temporel

---

## 11. Context-Dependent Phones

### Principe
- Un phonÃ¨me dÃ©pend de son contexte
- ModÃ©lisation plus fine des transitions phonÃ©tiques

### BÃ©nÃ©fices
- Contraintes plus fortes sur le dÃ©codage
- RÃ©duction du Word Error Rate

*(implÃ©mentation simplifiÃ©e dans le projet)*

---

## 12. EntraÃ®nement discriminatif de sÃ©quence (sMBR)

### Limite de CE / CTC
- Optimisent une loss locale
- Pas directement le WER

### sMBR
- Optimisation directe au niveau sÃ©quence
- AmÃ©lioration significative des performances

*(discutÃ© conceptuellement)*

---

# ğŸ§  PARTIE VI â€“ EXTENSIONS ET OUVERTURES
*(VidÃ©o â€“ fin | Notebook 03)*

---

## 13. Extensions explorÃ©es

Exemples :
- Comparaison unidirectionnel vs bidirectionnel
- Effet du frame stacking
- CTC caractÃ¨re vs phonÃ¨me

Objectif :
> Explorer des variantes et analyser leur impact, mÃªme sans gain de performance.

---

## 14. Positionnement par rapport aux modÃ¨les modernes

- Attention-based models
- Transformers / Conformers
- RÃ´le fondamental de CTC dans les systÃ¨mes actuels

---

# ğŸ§  PARTIE VII â€“ CONCLUSION

---

## 15. Conclusion gÃ©nÃ©rale

- Les RNN/LSTM sont naturellement adaptÃ©s Ã  la parole
- CTC permet dâ€™apprendre lâ€™alignement automatiquement
- Les amÃ©liorations dâ€™ingÃ©nierie sont cruciales
- Le projet montre le passage complet :

