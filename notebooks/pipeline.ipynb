{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c01c6b",
   "metadata": {},
   "source": [
    "## Pipeline for Speech Recognition based on the ASR notebook\n",
    "\n",
    "*Remark :* The model has only been trained to recognize English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5717e8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1\n",
      "Torcheval version: 0.0.7\n",
      "Torchaudio version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torcheval\n",
    "import torchaudio\n",
    "from IPython.display import Audio\n",
    "\n",
    "import soundfile\n",
    "import sounddevice\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Torcheval version: {torcheval.__version__}\")\n",
    "print(f\"Torchaudio version: {torchaudio.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03d57e9",
   "metadata": {},
   "source": [
    "### Asking the user to speak and to record it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c209b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data folder if it doesn't exists\n",
    "if not os.path.exists('../data/live_input'):\n",
    "    os.makedirs('../data/live_input')\n",
    "\n",
    "def record_until_enter(filename=\"../data/live_input/audio.flac\", fs=16000):\n",
    "    recorded_audio = []\n",
    "\n",
    "    def callback(indata, frames, time, status):\n",
    "        if status:\n",
    "            print(status)\n",
    "        recorded_audio.append(indata.copy())\n",
    "\n",
    "    # Opening audio flux\n",
    "    with sounddevice.InputStream(samplerate=fs, channels=1, callback=callback):\n",
    "        print(\"-\"*40)\n",
    "        print(\"RECORDING...\")\n",
    "        print(\"Press [ENTER] to stop\")\n",
    "        input() \n",
    "\n",
    "    # Concatenating the audio\n",
    "    full_recording = np.concatenate(recorded_audio, axis=0)\n",
    "    \n",
    "    # Saving\n",
    "    soundfile.write(filename, full_recording, fs)\n",
    "    print(\"-\"*40)\n",
    "    print(\"FILE SAVED\")\n",
    "    print(\"-\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11715d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 16000\n",
    "win_length = int(0.025 * sample_rate)               # normalized win_length to have the number of samples\n",
    "hop_length = int(0.01 * sample_rate)                # same for the stride\n",
    "n_fft = 2 ** int(np.ceil(np.log2(win_length)))      # highest power of 2 that can fit in the window\n",
    "n_mels = 80                                         # 80 dimensions features \n",
    "\n",
    "wave_to_mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=sample_rate, \n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    n_mels=n_mels      \n",
    ")\n",
    "\n",
    "spectrogram_to_log = torchaudio.transforms.AmplitudeToDB()\n",
    "\n",
    "# Pipeline extracting the features\n",
    "extracting_pipeline = torch.nn.Sequential(\n",
    "    wave_to_mel_spectrogram,\n",
    "    spectrogram_to_log\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "930f55f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the vocabulary used to map the text to numbers\n",
    "class TextTransform:\n",
    "    def __init__(self) -> None:\n",
    "        # We map text to integers (and reverse) using this coding \n",
    "        # The library LibriSpeech is already normalized, we're using it's norm \n",
    "        # 0 -> Blank\n",
    "        # 1 -> '\n",
    "        # 2 -> Space\n",
    "        # 3 -> a ...\n",
    "\n",
    "        self.__char_map = {}\n",
    "        self.__index_map = {}\n",
    "\n",
    "        # First the blank, ' and space \n",
    "        self.__char_map[\"<BLANK>\"] = 0\n",
    "        self.__index_map[0] = \"<BLANK>\"\n",
    "        self.__char_map[\"'\"] = 1\n",
    "        self.__index_map[1] = \"'\"\n",
    "        self.__char_map[\" \"] = 2\n",
    "        self.__index_map[2] = \" \"\n",
    "\n",
    "        # The rest of the characters\n",
    "        for i, char in enumerate(\"abcdefghijklmnopqrstuvwxyz\"):\n",
    "            self.__char_map[char] = i + 3\n",
    "            self.__index_map[i + 3] = char\n",
    "\n",
    "    def text_to_int(self, text) -> list[int]:\n",
    "        # Mapping the text to integers\n",
    "        int_list = []\n",
    "        for char in text.lower():\n",
    "            if char in self.__char_map:\n",
    "                int_list.append(self.__char_map[char])\n",
    "        return int_list\n",
    "        \n",
    "    def int_to_text(self, list) -> str:\n",
    "        # Mapping the integers to text\n",
    "        text = \"\"\n",
    "        for integer in list:\n",
    "            idx = integer.item() if hasattr(integer, \"item\") else integer\n",
    "            text += self.__index_map[idx]\n",
    "        return text\n",
    "    \n",
    "text_transform = TextTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be772e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoding(output, labels=None, label_lengths=None):\n",
    "    arg_maxes = torch.argmax(output, dim=2) # output shape [Batch, Time, Vocabulary] \n",
    "    decodes = []\n",
    "    targets = []\n",
    "\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "        decode = []\n",
    "\n",
    "        # Removing consecutive similar sample\n",
    "        pred_integers = torch.unique_consecutive(args)\n",
    "\n",
    "        # Removing blank \n",
    "        for token in pred_integers:\n",
    "            if token != 0: # Blank token\n",
    "                decode.append(token)\n",
    "\n",
    "        decode_str = text_transform.int_to_text(decode)\n",
    "        decodes.append(decode_str)\n",
    "\n",
    "        # Decoding the target label if given\n",
    "        if labels is not None and label_lengths is not None:\n",
    "            target_length = label_lengths[i]\n",
    "            target_label = labels[i][:target_length].tolist() # Ignoring padding in the label\n",
    "            target_str = text_transform.int_to_text(target_label) \n",
    "            targets.append(target_str)\n",
    "\n",
    "    if labels is not None:\n",
    "        return decodes, targets\n",
    "    else:\n",
    "        return decodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75be88d7",
   "metadata": {},
   "source": [
    "### Different models available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d3a4eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dim : number of features per window => 80\n",
    "# hidden_dim : 512 or 256 for bidirectional => in article normally 500 and 300, but I prefer power of 2 \n",
    "# output_dim : vocabulary size + 1 token blank space => for computational reasons we take 29\n",
    "# n_layers : 5 according to the article\n",
    "\n",
    "class SpeechRecognition(nn.Module):\n",
    "    def __init__(self, input_dim : int, hidden_dim : int, output_dim :int, n_layers : int, bidirectional : bool) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.input_dim,          # Input dim\n",
    "            hidden_size=self.hidden_dim,        # Hidden dim\n",
    "            num_layers=self.n_layers,           # Num layers\n",
    "            bidirectional=self.bidirectional,   # Bidirectional\n",
    "            batch_first=True,                   # Batch first\n",
    "            dropout=0.1                         # To avoid overfitting\n",
    "        )\n",
    "        \n",
    "        # Output dim - if bidirectional output size 2 time longer\n",
    "        lstm_output_dim = self.hidden_dim * 2 if bidirectional else self.hidden_dim\n",
    "        \n",
    "        # Output layer for classification\n",
    "        self.classifier = nn.Linear(lstm_output_dim, self.output_dim)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        # x : shape (Batch, Time, input_dim)\n",
    "        output, _ = self.lstm(x)                    # output : shape (Batch, Time, lstm_output_dim)\n",
    "        logits = self.classifier(output)            # logits : shape (Batch, Time, output_dim) -> output_dim = size vocabulary, taking dim=2 in log_softmax to compute proba on this and not batch or time\n",
    "        log_proba = F.log_softmax(logits, dim=2)    # we normalize the logits to have probability and take log of these, to avoid numerical 0\n",
    "        return log_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5cdb064",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechRecognitionStacking(nn.Module):\n",
    "    def __init__(self, input_dim : int, hidden_dim : int, output_dim :int, n_layers : int, bidirectional : bool, stride : int, stack : int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # Updating with stacking\n",
    "        self.stride = stride\n",
    "        self.stack = stack\n",
    "\n",
    "        stacked_dim = self.stack * input_dim\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=stacked_dim,             # Input dim\n",
    "            hidden_size=self.hidden_dim,        # Hidden dim\n",
    "            num_layers=self.n_layers,           # Num layers\n",
    "            bidirectional=self.bidirectional,   # Bidirectional\n",
    "            batch_first=True,                   # Batch first\n",
    "            dropout=0.1                         # To avoid overfitting\n",
    "        )\n",
    "        \n",
    "        # Output dim - if bidirectional output size 2 time longer\n",
    "        lstm_output_dim = self.hidden_dim * 2 if bidirectional else self.hidden_dim\n",
    "        \n",
    "        # Output layer for classification\n",
    "        self.classifier = nn.Linear(lstm_output_dim, self.output_dim)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        # Padding if sample too short\n",
    "        if x.size(1) < self.n_stack:\n",
    "            padding = torch.zeros(x.size(0), self.n_stack - x.size(1), x.size(2)).to(device)\n",
    "            x = torch.cat([x, padding], dim=1)\n",
    "\n",
    "        # Output shape temporary : (Batch, New_Time, Features, Stack_Size)\n",
    "        x = x.unfold(dimension=1, size=self.n_stack, step=self.n_skip)\n",
    "        \n",
    "        # We get (Batch, New_Time, Features * Stack_Size) -> (Batch, T', 640)\n",
    "        batch, new_time, feats, stack = x.size()\n",
    "        x = x.contiguous().view(batch, new_time, feats * stack)\n",
    "        \n",
    "        # x : shape (Batch, Time, input_dim)\n",
    "        output, _ = self.lstm(x)                    # output : shape (Batch, Time, lstm_output_dim)\n",
    "        logits = self.classifier(output)            # logits : shape (Batch, Time, output_dim) -> output_dim = size vocabulary, taking dim=2 in log_softmax to compute proba on this and not batch or time\n",
    "        log_proba = F.log_softmax(logits, dim=2)    # we normalize the logits to have probability and take log of these, to avoid numerical 0\n",
    "        return log_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a492a2f",
   "metadata": {},
   "source": [
    "### Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bd18144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LOADING MODEL ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0x/tnwyycrs76jb4wzv1753ymx40000gn/T/ipykernel_18397/621397275.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  backup = torch.load(f\"../model/{model_version}/{model_version}_final.pth\", map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "#elif torch.backends.mps.is_available(): \n",
    "#    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "print(\"\\n--- LOADING MODEL ---\")\n",
    "\n",
    "model_version = \"V4\" # Code to know to which model it refers \n",
    "backup = torch.load(f\"../model/{model_version}/{model_version}_final.pth\", map_location=device)\n",
    "\n",
    "model = SpeechRecognition(\n",
    "    input_dim=80,\n",
    "    hidden_dim=256,\n",
    "    output_dim=29, \n",
    "    n_layers=5,\n",
    "    bidirectional=True\n",
    ")\n",
    "\n",
    "model.load_state_dict(backup['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf60517",
   "metadata": {},
   "source": [
    "### Recording if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ba698c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "RECORDING...\n",
      "Press [ENTER] to stop\n",
      "RECORDING STOPPED\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "FILE SAVED\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Recording \n",
    "record_until_enter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bcf14f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "DECODING ...\n",
      "\n",
      "Text : HEIRS RAE\n",
      "\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 40)\n",
    "print(\"DECODING ...\\n\")\n",
    "\n",
    "# Loading the save\n",
    "my_wave, _ = torchaudio.load(\"../data/live_input/audio.flac\")\n",
    "\n",
    "# Extract features\n",
    "spec = extracting_pipeline(my_wave)\n",
    "\n",
    "# 3. Add Batch Dimension \n",
    "# Models expect input shape (Batch, Time, Features)\n",
    "# Since we loaded a single file, we must add a batch dimension of 1.\n",
    "if spec.dim() == 2: \n",
    "    spec = spec.unsqueeze(0)  # Shape becomes (1, freq, time)\n",
    "spec = spec.transpose(1, 2)\n",
    "\n",
    "spec = spec.to(device)\n",
    "\n",
    "# Set Model to Evaluation Mode\n",
    "model.eval()\n",
    "\n",
    "# Forward pass with no_grad\n",
    "with torch.no_grad():\n",
    "    log_probs = model(spec)\n",
    "\n",
    "# Output \n",
    "text = greedy_decoding(log_probs)[0]\n",
    "\n",
    "print(\"Text : \" + text.upper())\n",
    "print(\"\\n\"+\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech_recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
